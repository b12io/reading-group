# Reading
*Language Models are Few-Shot Learners (The GPT-3 paper)*

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, and 27 others!

- [Original paper](https://arxiv.org/pdf/2005.14165.pdf)
- [GPT-3 explained in under 2 minutes](https://towardsdatascience.com/gpt-3-explained-in-under-2-minutes-9c977ccb172f), as simple an explanation of what GPT-3 is as you’ll get.
- [A hands-on account of working with the model](https://minimaxir.com/2020/07/gpt3-expectations/), with some summary of the twitter hype.
- [A summary of the GPT-3 paper](https://lambdalabs.com/blog/demystifying-gpt-3/), with some commentary on scalability
- [A nice detailed walkthrough of how transformer models work](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04) (the class of model to which GPT-3 belongs)
- [A hitchiker’s guide (!) to GPT-3](https://lambdalabs.com/blog/gpt-3/), mostly great links to even further reading. If you’re a visual learner or want to look at cool things, scroll to the bottom of the page for “The best videos on GPT-3" and “Cool GPT-3 demos”.

# Questions I had before reading the paper
- How does GPT-3 work?
- What is novel about this version of the model, and what makes it different from GPT-2 and other models that have come before?
- What insights can we gain into what leads to specific model outputs?
- Twitter told me this model was very expensive to train. Is there a future where it becomes cheap, or are generative ML models only trainable by massive companies?

# Takeaways
- GPT-3 is a "transformer model" very similar to previous models like its predecessor GPT-2 or BERT.
- Transformer models work by transforming input sequences into predicted output sequences, leveraging an 'attention' mechanism to add additional context during the
  transformation. This is different from 'memory'-based models that store previously seen words in the sequence via recurrent neural network connections (loops in the network).
- The main differentiator for GPT-3 is its size: it has 175 billion parameters (compared to GPT-2's 1.5 billion parameters), and was trained on 499 billion examples
  (taken from publicly-available corpora of text on the web).
- This raises questions about how the approach can scale: the model costs at least $4.6 million to train, and the authors showed a power-law relationship between
  model performance and model size/data volume/computation that might not be sustainable.
- As a result of its scale, GPT-3 is able to be more 'general-purpose' than other models, solving novel tasks without a fine-tuning step.
- Humans are only able to guess that GPT-3 generated text wasn't written by a human 52% of the time (barely better than random chance).
- The demos of other tasks beyond text generation in action are awesome! They result from cleverly giving the model a prompt containing one or a few examples to
  "teach" it the desired output format.
- However, the model is a black box, and outputs are frequently poor or (worse) inappropriate as a result of the training data, which raises questions about how
  best to use the model's outputs.

# Notes
Paper by paper: 

## [GPT-3 explained in under 2 minutes](https://towardsdatascience.com/gpt-3-explained-in-under-2-minutes-9c977ccb172f)
- GPT-3 is a neural-network-powered language model that predicts the likelihood of a sentence existing in the world.
- It is trained on the [Common Crawl](https://commoncrawl.org/) dataset, where an example is generated by removing random words from the corpus and trying to predict
  the removed words from surrounding text.
- The model architecture is a Transformer-based neural network, similar to the architecture of BERT, another leading NLP model.
- Its main difference from BERT and other transform-based models is its size: 175 billion parameters, compared to GPT-2's 1.5 parameters.
- This size allows the model to be more 'general-purpose' than other models, without requiring fine-tuning the way BERT does.

## [A hands-on account of working with the model](https://minimaxir.com/2020/07/gpt3-expectations/)
- GPT-3 is only available via API access, not only because it's in private beta, but because the model is so large it needs a custom cluster to run on.
- If you have access, you can hit the API with an HTTP request containing a) a prompt to start from and b) the number of tokens to return. You'll get back text that
  could come after the prompt. (Of course, your prompt doesn't need to be an English sentence, it could be a question, or another language, or code, etc.)
- The model has an additional 'temperature' parameter, which allows it to output sub-optimal ("less likely to be a real sentence") predictions.
  High-temperature predictions tend to be more creative, but at greater risk for not making sense.
- After the model's release, there has been a viral storm of cool demos, from full generated articles to clever tweets to generated React applications.
- GPT-3 was trained in October 2019 and doesn't know about COVID yet :-p.
- Because all GPT-3 takes is a prompt, the trick to using it well is devising clever prompts that can *reliably* generate outputs in the desired format.
- Practical problems with the model include latency (serving predictions can be slow), over-confidence in the model's power due to selection bias
  (all the Twitter hype only showed you the best outputs, not the average outputs), and the fact that there's only one trained model with one corpus, which means
  that there's no way to gain competitive advantage by fine-tuning the model. (My 2c is that none of these is a blocker to creative applications!)
- A more serious problem is that the model can produce racist and sexist outputs, and nothing in the model attempts to mitigate bias. The OpenAI web UI implements 
  toxicity detectors, but if you're working directly with the API, you're on your own.
- Interesting question: who owns the copyright to content generated by OpenAI's API?

## [A summary of the GPT-3 paper](https://lambdalabs.com/blog/demystifying-gpt-3/)
- Model performance scales as a power-law of model size, dataset size, and computation
- Trained on enough data, a model can solve NLP tasks it has never encountered, without fine-tuning
- The costs of training the model are enormous: $4.6 Million using publicly available cloud hardware.
- The state of the art of language models are 10x larger each year, significantly faster than GPU memory capacity growth.
- GPT-3 is a black box: even the researchers don't understand how the model accomplishes novel tasks.
- The model uses the same attention-based architecture as GPT-2. The biggest model uses 96 attention layers, each with 96x128-dimension heads.
  It is trained on 499 B tokens.
- Training data comes from a number of sources, including Common Crawl, WebText2, wikipedia, and more.
- The data compression ratio (# training examples / # parameters) is lower for GPT-3 vs. GPT-2, suggesting that GPT-3 might function by encoding more
  training data in the model and pattern-matching on it during inference.
- Interesting issue when working with large datasets: contamination. Due to source overlap, you might accidentally have test data duplicated in your training set.
  They eliminated all 13-gram overlaps between test and training text, and it didn't make a difference on most task, but it did on a few tasks!
- GPT-3 175B model required 3.14E23 FLOPS of computing for training.
- Models were trained on V100 GPUs on a high-bandwidth cluster, and tricks like model-parallelism within matrix multiplies and model-parallelism across network
  layers were needed to avoid running out of memory.
- Eval: at the text generation task, humans could only guess which articles were human-written 52% of the time (compared to 76% for a smaller 125M parameter model)
- Beyond text generation and other common NLP tasks like translation, the model is suprisingly good at novel tasks like arithmetic computation.
- How does the model work under the hood? It seems to do well on language modeling tasks, and less well on reasoning tasks.
- And how do the examples provided in the prompt help? Two theories: they "teach" the model how to reason, or they "filter" the model to focus on a relevant
  subset of its training data. The filter hypothesis feels more plausible, given that GPT-3 performs poorly on reasoning tasks. One exception is arithmetic, but
  even that has interesting failure modes: it can add 4-digit numbers, but not 5-digit numbers.
- The human brain has 100 trillion synapses. We're still 3 orders of magnitude short with 175B parameters, but it no longer feels impossible to have a model of the
  size of the brain! (But would it work as well?)
  
## [A nice detailed walkthrough of how transformer models work](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)
GPT-3 is a transformer model. Here's a transformer model archiecture:
<img src="https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png" width="500"/>

Here are notes on how it works:
- A Seq2Seq neural-net model takes a sequence as input, and outputs another sequence.
- Seq2Seq models are good at handling tasks where data is text, since they naturally encode the order of the input sequence.
- A Seq2Seq model has an encoder, which maps input sequence to a vector space, and a decoder, which maps a vector in that vector space to an output sequence
- A common choice is to use a single LSTM as the encoder, and another LSTM as the decoder. An LSTM (long-short-term memory) model is an RNN
  (recurrent neural network) that uses a 'memory' mechanism to keep recent sequence items fresh.
- A transformer model is a Seq2Seq model that incorporates a notion of 'attention'
- The attention mechanism, at each token in an input sequence, decides which other parts of the sequence are 'important'.
- Essentially, this means that the decoder, in addition to receiving a vector produced by the encoder, receives a set of encoded tokens from throughout the sequence.
- Each attention token comes with a weight, which the decoder takes into account when producing the output vector
- Transformer models like BERT use only the attention mechanism, and no recurrent memory mechanism.
- To avoid the need for memory (which helps recall where a word appeared in the sequence), the transformer model adds a 'relative position in the string'
  dimension to the base encoding of words as vectors
- Mathematically, attention is essentially a matrix product of the representation of an individual word in the sequence with the representations of all the words
  in the sequence, normalized so that each word gets a weight between 0 and 1 for "how influential it is to the rest of the sequence".
- In a transformer model, the attention layer is run many times in parallel on various linearly-transformed copies of the input sequence to let it use multiple
  representations of the data during inference. The linear transformations are stored as weight matrices W, which are what get updated during training.
- The attention layer is applied to inputs and outputs, then merged together and pushed through a standard feed-forward neural network.
- Training the network is like any other neural network: the goal is to learn all the weight matrices in the architecture.
- To train, feed the network an input sequence to the encoder, and its matching output sequence to the decoder (shifted right by one word so that the network is
  trained on having seen only the first i - 1 output words when predicting the ith word.
- Additionally, implement a 'masking mechanism' during training so that the decoder only gets to use the first i-1 output words in the attention mechanism matrix
  multiplications.
- During inference, since we don't have the decoder input yet, construct it iteratively: run the network on the input sequence plus an empty 'start of sentence'
  token, then take the first word of output and re-run the network on the input sequence plus an output sequence composed of 'start of sentence' + 'first word'.
  Continue generating output tokens until the network predicts an 'end of sentence' token.
  
# Questions coming out?
- The size of the training set was important to the power of GPT-3, but how important was it's quality?
  What other data could be used to train future iterations of the model?
- Would adding fine-tuning to the model help us understand how it works better? If not, what's the path towards actually understanding what GPT-3 does?
- What are the implications of a model that can fool humans 50% of the time, but won't reliably generate factual text,
  especially amid current fears around deep fakes and fake news?
- Is the "power-law scaling of massive models that get better and better at simulating human tasks" the future of AI? Is it the fastest path to
  "generalized intelligence", and is it even heading there at all?

# Questions / Applications for B12
- Some B12ers already played with the mode a bit during the hackathon for text generation. What other uses for GPT-3 can we imagine in our product?
- In general, what makes a good use case for GPT-3, given that we don't have reliable guarantees on output quality / sensibiity?
- How do generative models that output fully written text or fully formed HTML fit into B12's philosophy around using automation to augment creative humans?
- Given that GPT-3 is a product of its training data and has been observed to generate racist or inappropriate outputs, what is our ethical responsibility
  when building the model into interfaces used by experts or customers?
