# Reading
Normally we read papers, but we wanted a more accessible introduction to neural networks, so we read
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1)

# 3 Takeaways


# Questions

# Other references
* [The code behind the RNN post](https://github.com/karpathy/char-rnn)
* [The char-rnn repository referenced a paper on dropout, and I didn't know what that was](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The short story: If you're training a network with lots of units will require lots of parameters to tune, which, if you don't have enough data, could result in overfitting. Dropout randomly removes units during training steps.
