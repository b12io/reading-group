# Reading: Using Rubrics for Crowdsourced Design Critique
* [Almost an Expert: The Effects of Rubrics and Expertise on Perceived Value of Crowdsourced Design Critiques](http://www.cs.cmu.edu/~spdow/files/CrowdCrit-rubrics-cscw2016.pdf)


# 3 Takeaways
(marcua rough notes)
* Design rubrics help everyone
* Help novices more than experts
* Experts still provide better justification
(paopow takeaways on top of marcau's)
* Better justification => more helpful feedback
* Writing styles affects the perceived helpfulness

# Questions I have going in
* How can we improve the quality of feedback given by novices?
* What are the attributes of helpful feedbacks? How can we ensure the feedbacks of certain quality?

# Overall notes
* Helpful feedbacks are positive, specific and justifiable.
* Measures of feedback quality
  - Rating by feedback receivers
  - Rating of revised artifacts
  - Contrasting with feedback from experts
* 2x2 experiment
  - Independent variables: expertise (expert, novice) and rubrics (rubric, no rubric)
  - Dependent variables: feedback ratings of the students who received feedbacks
  - Covariants: student raters' design experience and vocabularly richness
* Active sentences ("Why don't you increase the constrast?") are rated as more helpful than non-active statement ("The constrast is low.")
* Experts provide feedbacks with clearer justifications.
* Repeated multiple feedbacks help with prioritization

# Questions I have coming out
* How do we ensure that the generated feedbacks are good feedback?
* How do we build good design rubrics?
* Could _bad_ rubrics harm the feedback?
* If gathering feedbacks from multiple sources is helpful, how can we scalably achieve it?
* How does the perceived helpfulness correlate with improvement in design?
* How much does the perceived helpfulness depend on personal traits of the receiver?
* How can automation help with feedback providing?

# Questions for B12 to ponder
* We should provide rubrics to our design reviewers
* We should provide rubrics to our customers
* We should support customers in providing helpful rationale for their feedbacks
* Experts should be able to communicate justification of their solutions to the customers
* Our designer-powered recommendations are a form of design critique as well. What can we learn from that?
* CrowdCrit provides feedback with visual annotations---until we have that, how will our results be different?
* Can we open our dataset to one of these researchers?


# Other references
* [The original CrowdCrit Paper](http://www.cs.cmu.edu/~spdow/files/CrowdCrit-cscw2015.pdf)
* [CritViz, another reference](http://www.ieeetclt.org/issues/january2013/Tinapple.pdf)
* [Voyant: Generating Structured Feedback on Visual
Designs Using a Crowd of Non-Experts](https://pdfs.semanticscholar.org/87d9/5dc96c90e1e371ffc484ac6bfa83a3be75b5.pdf)
